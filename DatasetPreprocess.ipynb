{"cells":[{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"p5gDz8hhwuU9"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"2PmlnmmJmG_I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737594934753,"user_tz":-60,"elapsed":10,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"4d23b217-329b-44ce-9894-6d1a9d9a41c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Niet in Google Colab, slaan Google Drive-mount over.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.feature_selection import chi2, SelectKBest, RFE\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, SGDClassifier\n","from sklearn.dummy import DummyClassifier\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    classification_report,\n","    silhouette_score,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_auc_score\n",")\n","import matplotlib.pyplot as plt\n","import logging\n","import re\n","from urllib.parse import urlparse\n","\n","# Configure logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"Google Drive is gekoppeld!\")\n","except ModuleNotFoundError:\n","    print(\"Niet in Google Colab, slaan Google Drive-mount over.\")\n","\n"]},{"cell_type":"markdown","source":["#Controle"],"metadata":{"id":"5r789YmOwxj_"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","\n","def count_html_files(pad):\n","    return sum(f.lower().endswith(('.html', '.htm')) for f in os.listdir(pad))\n","\n","dataset_paden = {\n","    \"Benign 2018\": \"C:/Users/cihat/Downloads/Phishing_dataset1/benign/2018/2018\",\n","    \"Benign 2019\": \"C:/Users/cihat/Downloads/Phishing_dataset1/benign/2019/2019\",\n","    \"Benign 2020\": \"C:/Users/cihat/Downloads/Phishing_dataset1/benign/2020/2020\",\n","    \"Malicious 2018\": \"C:/Users/cihat/Downloads/Phishing_dataset1/malicious/2018/2018\",\n","    \"Malicious 2019\": \"C:/Users/cihat/Downloads/Phishing_dataset1/malicious/2019/2019\",\n","    \"Malicious 2020\": \"C:/Users/cihat/Downloads/Phishing_dataset1/malicious/2020/2020\",\n","}\n","\n","print(\"Aantal HTML-bestanden per jaar:\")\n","for naam, pad in dataset_paden.items():\n","    if os.path.exists(pad):\n","        print(f\"{naam}: {count_html_files(pad)} bestanden\")\n","    else:\n","        print(f\"{naam}: Pad niet gevonden\")\n","\n","data_path = \"C:/Users/cihat/Downloads/Phishing_dataset1/\"\n","npy_files = []\n","\n","# Enkel zoeken naar .npy-bestanden\n","for root, _, files in os.walk(data_path):\n","    for file in files:\n","        if file.endswith(\".npy\"):\n","            npy_files.append(os.path.join(root, file))\n","\n","print(\"\\nGevonden .npy bestanden:\")\n","print(\"\\n\".join(npy_files) if npy_files else \"Geen .npy bestanden gevonden.\")\n","\n","if npy_files:\n","    print(\"\\nLaden van .npy bestanden...\")\n","    for file in npy_files:\n","        data = np.load(file)\n","        print(f\"Bestand: {file}, Vorm: {data.shape}\")\n"],"metadata":{"id":"EkKAlT4EomK_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737381471530,"user_tz":-60,"elapsed":13844,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"2cec7ef0-b8f4-45c0-cd91-609b4eae7fd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Aantal HTML-bestanden per jaar:\n","Benign 2018: 17462 bestanden\n","Benign 2019: 18521 bestanden\n","Benign 2020: 92068 bestanden\n","Malicious 2018: 16063 bestanden\n","Malicious 2019: 65075 bestanden\n","Malicious 2020: 158370 bestanden\n","\n","Gevonden .npy bestanden:\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2018_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2019_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2020_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2018_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2019_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2020_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2018_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2019_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2020_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2018_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2019_balanced.npy\n","C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2020_balanced.npy\n","\n","Laden van .npy bestanden...\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2018_balanced.npy, Vorm: (15853, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2019_balanced.npy, Vorm: (18039, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_benign_2020_balanced.npy, Vorm: (87762, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2018_balanced.npy, Vorm: (15781, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2019_balanced.npy, Vorm: (16067, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\features_malicious_2020_balanced.npy, Vorm: (72785, 300)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2018_balanced.npy, Vorm: (15853,)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2019_balanced.npy, Vorm: (18039,)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_benign_2020_balanced.npy, Vorm: (87762,)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2018_balanced.npy, Vorm: (15781,)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2019_balanced.npy, Vorm: (16067,)\n","Bestand: C:/Users/cihat/Downloads/Phishing_dataset1/output\\labels_malicious_2020_balanced.npy, Vorm: (72785,)\n"]}]},{"cell_type":"markdown","source":["#Feature Extractie & Data balancering"],"metadata":{"id":"_CovDlj6w2Yt"}},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","\n","print(\"Model downloaden en inladen...\")\n","fasttext_model = api.load('fasttext-wiki-news-subwords-300')  # Engels model\n","print(\"Model geladen!\")\n","\n","# Test het model\n","word_vector = fasttext_model['example']  # Vervang \"example\" door een woord\n","print(\"Vector voor 'example':\", word_vector[:10])  # Eerste 10 dimensies"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LH_vhcLATylo","executionInfo":{"status":"ok","timestamp":1734472853194,"user_tz":-60,"elapsed":211658,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"1fe839eb-cd6e-46d5-f306-c22a1be7b5a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in c:\\users\\cihat\\miniconda\\miniconda3\\lib\\site-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\cihat\\miniconda\\miniconda3\\lib\\site-packages (from gensim) (1.24.3)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\cihat\\miniconda\\miniconda3\\lib\\site-packages (from gensim) (1.11.2)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\cihat\\miniconda\\miniconda3\\lib\\site-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in c:\\users\\cihat\\miniconda\\miniconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.15.0)\n","Model downloaden...\n","Model geladen!\n","Vector voor 'example': [ 0.0073217 -0.018045   0.038147  -0.0031285 -0.058175  -0.0020996\n"," -0.0076067 -0.12056   -0.081749  -0.039819 ]\n"]}]},{"cell_type":"markdown","source":["##2018"],"metadata":{"id":"5bk-6CLexJkc"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from bs4 import BeautifulSoup\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","import chardet\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm  # Voor voortgangsindicator\n","\n","# Voorgetraind FastText-model laden\n","print(\"Voorgetraind FastText-model downloaden en laden...\")\n","fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n","print(\"FastText-model succesvol geladen!\")\n","\n","# Basisdatasetpaden\n","benign_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\benign\\2018\\2018\")\n","malicious_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\malicious\\2018\\2018\")\n","output_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\")\n","output_path.mkdir(parents=True, exist_ok=True)\n","\n","# Logbestand voor errors\n","log_file = output_path / \"error_log_2018.txt\"\n","\n","def log_error(message):\n","    with open(log_file, 'a') as log:\n","        log.write(f\"{message}\\n\")\n","\n","# Functie om tekst uit HTML-bestanden te extraheren\n","def extract_text_from_html(file_path):\n","    try:\n","        with open(file_path, 'rb') as f:\n","            raw_data = f.read()\n","            detected_encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'\n","        with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as file:\n","            soup = BeautifulSoup(file, 'lxml')\n","            return soup.get_text(separator=\" \", strip=True)\n","    except Exception as e:\n","        log_error(f\"Fout bij bestand {file_path}: {e}\")\n","        return None\n","\n","# Functie om FastText-vectoren te genereren\n","def generate_fasttext_vectors(text, model):\n","    words = text.split()\n","    vectors = [model[word] for word in words if word in model.key_to_index]\n","    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n","\n","# Functie om bestanden veilig te verwerken\n","def process_html_file(file_path, label):\n","    try:\n","        text = extract_text_from_html(file_path)\n","        if text:\n","            vector = generate_fasttext_vectors(text, fasttext_model)\n","            return vector, label\n","    except Exception as e:\n","        log_error(f\"Fout bij verwerking van bestand {file_path}: {e}\")\n","    return None, None\n","\n","# Parallelle verwerking\n","def extract_features_parallel(file_paths, label):\n","    features = []\n","    labels = []\n","\n","    print(f\"{len(file_paths)} bestanden gevonden.\")\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        results = list(tqdm(executor.map(lambda f: process_html_file(f, label), file_paths),\n","                            total=len(file_paths), desc=\"Bezig met verwerken\"))\n","\n","    for vector, lbl in results:\n","        if vector is not None:\n","            features.append(vector)\n","            labels.append(lbl)\n","\n","    return np.array(features), np.array(labels)\n","\n","# Selecteer bestanden\n","benign_files = list(benign_path.glob(\"*.htm*\"))\n","malicious_files = list(malicious_path.glob(\"*.htm*\"))\n","\n","# Beperk malicious bestanden tot het aantal benign bestanden\n","min_size = min(len(benign_files), len(malicious_files))\n","benign_files = benign_files[:min_size]\n","malicious_files = malicious_files[:min_size]\n","\n","print(f\"Aantal benign bestanden: {len(benign_files)}\")\n","print(f\"Aantal malicious bestanden: {len(malicious_files)}\")\n","\n","# Verwerk benign bestanden\n","print(\"Feature-extractie voor benign bestanden starten...\")\n","features_benign, labels_benign = extract_features_parallel(benign_files, label=0)\n","\n","# Verwerk malicious bestanden\n","print(\"Feature-extractie voor malicious bestanden starten...\")\n","features_malicious, labels_malicious = extract_features_parallel(malicious_files, label=1)\n","\n","# Sla de gebalanceerde features en labels apart op\n","np.save(output_path / \"features_benign_2018_balanced.npy\", features_benign)\n","np.save(output_path / \"labels_benign_2018_balanced.npy\", labels_benign)\n","np.save(output_path / \"features_malicious_2018_balanced.npy\", features_malicious)\n","np.save(output_path / \"labels_malicious_2018_balanced.npy\", labels_malicious)\n","\n","print(\"Balancering voltooid!\")\n","print(f\"Benign samples: {len(features_benign)}\")\n","print(f\"Malicious samples: {len(features_malicious)}\")\n","print(\"Vorm van benign features:\", features_benign.shape)\n","print(\"Vorm van malicious features:\", features_malicious.shape)\n","print(\"Problemen gelogd in:\", log_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_8xQWRVdzM-","executionInfo":{"status":"ok","timestamp":1734525844575,"user_tz":-60,"elapsed":7246474,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"03793139-12a2-4c0d-b71b-839e377263f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Voorgetraind FastText-model downloaden en laden...\n","FastText-model succesvol geladen!\n","Aantal benign bestanden: 16063\n","Aantal malicious bestanden: 16063\n","Feature-extractie voor benign bestanden starten...\n","16063 bestanden gevonden.\n"]},{"output_type":"stream","name":"stderr","text":["Bezig met verwerken: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16063/16063 [1:29:45<00:00,  2.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature-extractie voor malicious bestanden starten...\n","16063 bestanden gevonden.\n"]},{"output_type":"stream","name":"stderr","text":["Bezig met verwerken: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16063/16063 [22:35<00:00, 11.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Balancering voltooid!\n","Benign samples: 15853\n","Malicious samples: 15781\n","Vorm van benign features: (15853, 300)\n","Vorm van malicious features: (15781, 300)\n","Problemen gelogd in: \\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\\error_log_2018.txt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["##2019"],"metadata":{"id":"90_OcKs1xM3k"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from bs4 import BeautifulSoup\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","import chardet\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm  # Voor voortgangsindicator\n","\n","# Voorgetraind FastText-model laden\n","print(\"Voorgetraind FastText-model downloaden en laden...\")\n","fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n","print(\"FastText-model succesvol geladen!\")\n","\n","# Basisdatasetpaden\n","benign_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\benign\\2019\\2019\")\n","malicious_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\malicious\\2019\\2019\")\n","output_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\")\n","output_path.mkdir(parents=True, exist_ok=True)\n","\n","# Logbestand voor errors\n","log_file = output_path / \"error_log_2019.txt\"\n","\n","def log_error(message):\n","    with open(log_file, 'a') as log:\n","        log.write(f\"{message}\\n\")\n","\n","# Functie om tekst uit HTML-bestanden te extraheren\n","def extract_text_from_html(file_path):\n","    try:\n","        with open(file_path, 'rb') as f:\n","            raw_data = f.read()\n","            detected_encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'\n","        with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as file:\n","            soup = BeautifulSoup(file, 'lxml')\n","            return soup.get_text(separator=\" \", strip=True)\n","    except Exception as e:\n","        log_error(f\"Fout bij bestand {file_path}: {e}\")\n","        return None\n","\n","# Functie om FastText-vectoren te genereren\n","def generate_fasttext_vectors(text, model):\n","    words = text.split()\n","    vectors = [model[word] for word in words if word in model.key_to_index]\n","    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n","\n","# Functie om bestanden veilig te verwerken\n","def process_html_file(file_path, label):\n","    try:\n","        text = extract_text_from_html(file_path)\n","        if text:\n","            vector = generate_fasttext_vectors(text, fasttext_model)\n","            return vector, label\n","    except Exception as e:\n","        log_error(f\"Fout bij verwerking van bestand {file_path}: {e}\")\n","    return None, None\n","\n","# Parallelle verwerking\n","def extract_features_parallel(file_paths, label):\n","    features = []\n","    labels = []\n","\n","    print(f\"{len(file_paths)} bestanden gevonden.\")\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        results = list(tqdm(executor.map(lambda f: process_html_file(f, label), file_paths),\n","                            total=len(file_paths), desc=\"Bezig met verwerken\"))\n","\n","    for vector, lbl in results:\n","        if vector is not None:\n","            features.append(vector)\n","            labels.append(lbl)\n","\n","    return np.array(features), np.array(labels)\n","\n","# Selecteer bestanden\n","benign_files = list(benign_path.glob(\"*.htm*\"))\n","malicious_files = list(malicious_path.glob(\"*.htm*\"))\n","\n","# Beperk malicious bestanden tot het aantal benign bestanden\n","min_size = min(len(benign_files), len(malicious_files))\n","benign_files = benign_files[:min_size]\n","malicious_files = malicious_files[:min_size]\n","\n","print(f\"Aantal benign bestanden: {len(benign_files)}\")\n","print(f\"Aantal malicious bestanden: {len(malicious_files)}\")\n","\n","# Verwerk benign bestanden\n","print(\"Feature-extractie voor benign bestanden starten...\")\n","#features_benign, labels_benign = extract_features_parallel(benign_files, label=0)\n","\n","# Verwerk malicious bestanden\n","print(\"Feature-extractie voor malicious bestanden starten...\")\n","features_malicious, labels_malicious = extract_features_parallel(malicious_files, label=1)\n","\n","# Sla de gebalanceerde features en labels apart op\n","np.save(output_path / \"features_benign_2019_balanced.npy\", features_benign)\n","np.save(output_path / \"labels_benign_2019_balanced.npy\", labels_benign)\n","np.save(output_path / \"features_malicious_2019_balanced.npy\", features_malicious)\n","np.save(output_path / \"labels_malicious_2019_balanced.npy\", labels_malicious)\n","\n","print(\"Balancering voltooid!\")\n","print(f\"Benign samples: {len(features_benign)}\")\n","print(f\"Malicious samples: {len(features_malicious)}\")\n","print(\"Vorm van benign features:\", features_benign.shape)\n","print(\"Vorm van malicious features:\", features_malicious.shape)\n","print(\"Problemen gelogd in:\", log_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lz3NzVAYNeiW","executionInfo":{"status":"ok","timestamp":1734489275765,"user_tz":-60,"elapsed":3329171,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"0972854c-e413-4efe-9991-c4624db28586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Voorgetraind FastText-model downloaden en laden...\n","FastText-model succesvol geladen!\n","Aantal benign bestanden: 18521\n","Aantal malicious bestanden: 18521\n","Feature-extractie voor benign bestanden starten...\n","Feature-extractie voor malicious bestanden starten...\n","18521 bestanden gevonden.\n"]},{"output_type":"stream","name":"stderr","text":["Bezig met verwerken: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18521/18521 [51:55<00:00,  5.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Balancering voltooid!\n","Benign samples: 18039\n","Malicious samples: 16067\n","Vorm van benign features: (18039, 300)\n","Vorm van malicious features: (16067, 300)\n","Problemen gelogd in: \\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\\error_log_2019.txt\n"]}]},{"cell_type":"markdown","source":["##2020"],"metadata":{"id":"VrJziZirxOeA"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from bs4 import BeautifulSoup\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","import chardet\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm  # Voor voortgangsindicator\n","\n","# Voorgetraind FastText-model laden\n","print(\"Voorgetraind FastText-model downloaden en laden...\")\n","fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n","print(\"FastText-model succesvol geladen!\")\n","\n","# Basisdatasetpaden\n","benign_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\benign\\2020\\2020\")\n","malicious_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\malicious\\2020\\2020\")\n","output_path = Path(r\"\\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\")\n","output_path.mkdir(parents=True, exist_ok=True)\n","\n","# Logbestand voor errors\n","log_file = output_path / \"error_log_2020.txt\"\n","\n","def log_error(message):\n","    with open(log_file, 'a') as log:\n","        log.write(f\"{message}\\n\")\n","\n","# Functie om tekst uit HTML-bestanden te extraheren\n","def extract_text_from_html(file_path):\n","    try:\n","        with open(file_path, 'rb') as f:\n","            raw_data = f.read()\n","            detected_encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'\n","        with open(file_path, 'r', encoding=detected_encoding, errors='ignore') as file:\n","            soup = BeautifulSoup(file, 'lxml')\n","            return soup.get_text(separator=\" \", strip=True)\n","    except Exception as e:\n","        log_error(f\"Fout bij bestand {file_path}: {e}\")\n","        return None\n","\n","# Functie om FastText-vectoren te genereren\n","def generate_fasttext_vectors(text, model):\n","    words = text.split()\n","    vectors = [model[word] for word in words if word in model.key_to_index]\n","    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n","\n","# Functie om bestanden veilig te verwerken\n","def process_html_file(file_path, label):\n","    try:\n","        text = extract_text_from_html(file_path)\n","        if text:\n","            vector = generate_fasttext_vectors(text, fasttext_model)\n","            return vector, label\n","    except Exception as e:\n","        log_error(f\"Fout bij verwerking van bestand {file_path}: {e}\")\n","    return None, None\n","\n","# Parallelle verwerking\n","def extract_features_parallel(file_paths, label):\n","    features = []\n","    labels = []\n","\n","    print(f\"{len(file_paths)} bestanden gevonden.\")\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        results = list(tqdm(executor.map(lambda f: process_html_file(f, label), file_paths),\n","                            total=len(file_paths), desc=\"Bezig met verwerken\"))\n","\n","    for vector, lbl in results:\n","        if vector is not None:\n","            features.append(vector)\n","            labels.append(lbl)\n","\n","    return np.array(features), np.array(labels)\n","\n","# Selecteer bestanden\n","benign_files = list(benign_path.glob(\"*.htm*\"))\n","malicious_files = list(malicious_path.glob(\"*.htm*\"))\n","\n","# Beperk malicious bestanden tot het aantal benign bestanden\n","min_size = min(len(benign_files), len(malicious_files))\n","benign_files = benign_files[:min_size]\n","malicious_files = malicious_files[:min_size]\n","\n","print(f\"Aantal benign bestanden: {len(benign_files)}\")\n","print(f\"Aantal malicious bestanden: {len(malicious_files)}\")\n","\n","# Verwerk benign bestanden\n","print(\"Feature-extractie voor benign bestanden starten...\")\n","features_benign, labels_benign = extract_features_parallel(benign_files, label=0)\n","\n","# Verwerk malicious bestanden\n","print(\"Feature-extractie voor malicious bestanden starten...\")\n","features_malicious, labels_malicious = extract_features_parallel(malicious_files, label=1)\n","\n","# Sla de gebalanceerde features en labels apart op\n","np.save(output_path / \"features_benign_2020_balanced.npy\", features_benign)\n","np.save(output_path / \"labels_benign_2020_balanced.npy\", labels_benign)\n","np.save(output_path / \"features_malicious_2020_balanced.npy\", features_malicious)\n","np.save(output_path / \"labels_malicious_2020_balanced.npy\", labels_malicious)\n","\n","print(\"Balancering voltooid!\")\n","print(f\"Benign samples: {len(features_benign)}\")\n","print(f\"Malicious samples: {len(features_malicious)}\")\n","print(\"Vorm van benign features:\", features_benign.shape)\n","print(\"Vorm van malicious features:\", features_malicious.shape)\n","print(\"Problemen gelogd in:\", log_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emSuXHDFU4So","executionInfo":{"status":"ok","timestamp":1734518598094,"user_tz":-60,"elapsed":29321955,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"cb04c4f3-671d-4da2-a938-070eb5774a66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Voorgetraind FastText-model downloaden en laden...\n","FastText-model succesvol geladen!\n","Aantal benign bestanden: 93115\n","Aantal malicious bestanden: 93115\n","Feature-extractie voor benign bestanden starten...\n","93115 bestanden gevonden.\n"]},{"output_type":"stream","name":"stderr","text":["Bezig met verwerken: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93115/93115 [5:04:48<00:00,  5.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature-extractie voor malicious bestanden starten...\n","93115 bestanden gevonden.\n"]},{"output_type":"stream","name":"stderr","text":["Bezig met verwerken: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93115/93115 [2:59:09<00:00,  8.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Balancering voltooid!\n","Benign samples: 87762\n","Malicious samples: 72785\n","Vorm van benign features: (87762, 300)\n","Vorm van malicious features: (72785, 300)\n","Problemen gelogd in: \\\\?\\C:\\Users\\cihat\\Downloads\\Phishing_dataset1\\output\\error_log_2020.txt\n"]}]},{"cell_type":"markdown","source":["#Normaliseren & Data splitsing"],"metadata":{"id":"wptn0mI9xs8V"}},{"cell_type":"code","source":["from tensorflow.keras import models, layers\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score, log_loss, mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import os\n","\n","# Controleer en laad de datasets\n","def check_and_load_data(path, file_name):\n","    file_path = os.path.join(path, file_name)\n","    if not os.path.exists(file_path):\n","        raise FileNotFoundError(f\"Bestand niet gevonden: {file_path}\")\n","    return np.load(file_path)\n","\n","output_path = r\"/content/drive/MyDrive/Afstuderen/Deadline/output\"\n","# Features laden\n","X_benign_2018 = check_and_load_data(output_path, \"features_benign_2018_balanced.npy\")\n","X_benign_2019 = check_and_load_data(output_path, \"features_benign_2019_balanced.npy\")\n","X_benign_2020 = check_and_load_data(output_path, \"features_benign_2020_balanced.npy\")\n","X_phish_2018 = check_and_load_data(output_path, \"features_malicious_2018_balanced.npy\")\n","X_phish_2019 = check_and_load_data(output_path, \"features_malicious_2019_balanced.npy\")\n","X_phish_2020 = check_and_load_data(output_path, \"features_malicious_2020_balanced.npy\")\n","\n","# Labels laden\n","y_benign_2018 = check_and_load_data(output_path, \"labels_benign_2018_balanced.npy\")\n","y_benign_2019 = check_and_load_data(output_path, \"labels_benign_2019_balanced.npy\")\n","y_benign_2020 = check_and_load_data(output_path, \"labels_benign_2020_balanced.npy\")\n","y_phish_2018 = check_and_load_data(output_path, \"labels_malicious_2018_balanced.npy\")\n","y_phish_2019 = check_and_load_data(output_path, \"labels_malicious_2019_balanced.npy\")\n","y_phish_2020 = check_and_load_data(output_path, \"labels_malicious_2020_balanced.npy\")\n","\n","# Combineer benign en malicious data en labels\n","X_2018 = np.vstack([X_benign_2018, X_phish_2018])\n","X_2019 = np.vstack([X_benign_2019, X_phish_2019])\n","X_2020 = np.vstack([X_benign_2020, X_phish_2020])\n","\n","y_2018 = np.hstack([y_benign_2018, y_phish_2018])\n","y_2019 = np.hstack([y_benign_2019, y_phish_2019])\n","y_2020 = np.hstack([y_benign_2020, y_phish_2020])\n","\n","# Combineer data van alle jaren om de schaal te bepalen\n","X_all = np.vstack([X_2018, X_2019, X_2020])  # Combineer 2018, 2019, en 2020\n","\n","# Initialiseer de scaler en bereken de schaal over alle jaren\n","scaler = MinMaxScaler()\n","scaler.fit(X_all)  # Bereken X_min en X_max over de volledige dataset\n","\n","# Pas de schaal consistent toe op elk jaar\n","X_2018_scaled = scaler.transform(X_2018)\n","X_2019_scaled = scaler.transform(X_2019)\n","X_2020_scaled = scaler.transform(X_2020)\n","\n","# -------------------------\n","# Splits data in Train, Validation en Test sets met stratificatie\n","# -------------------------\n","def split_data(X, y, test_size=0.4, val_size=0.5, random_state=42):\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","        X, y, test_size=test_size, stratify=y, random_state=random_state\n","    )\n","    X_val, X_test, y_val, y_test = train_test_split(\n","        X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=random_state\n","    )\n","    return X_train, X_val, X_test, y_train, y_val, y_test\n","\n","X_train_2018, X_val_2018, X_test_2018, y_train_2018, y_val_2018, y_test_2018 = split_data(X_2018_scaled, y_2018)\n","X_train_2019, X_val_2019, X_test_2019, y_train_2019, y_val_2019, y_test_2019 = split_data(X_2019_scaled, y_2019)\n","X_train_2020, X_val_2020, X_test_2020, y_train_2020, y_val_2020, y_test_2020 = split_data(X_2020_scaled, y_2020)\n","\n","# Controleer de verdeling per set en jaar\n","def print_class_distribution(y, year, dataset_name):\n","    unique, counts = np.unique(y, return_counts=True)\n","    distribution = dict(zip(unique, counts))\n","    print(f\"{year} {dataset_name} verdeling: {distribution}\")\n","\n","for year, datasets in zip(\n","    [\"2018\", \"2019\", \"2020\"],\n","    [\n","        (y_train_2018, y_val_2018, y_test_2018),\n","        (y_train_2019, y_val_2019, y_test_2019),\n","        (y_train_2020, y_val_2020, y_test_2020),\n","    ],\n","):\n","    print_class_distribution(datasets[0], year, \"Train\")\n","    print_class_distribution(datasets[1], year, \"Validation\")\n","    print_class_distribution(datasets[2], year, \"Test\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJEER-gDUYX-","executionInfo":{"status":"ok","timestamp":1737082133231,"user_tz":-60,"elapsed":29708,"user":{"displayName":"C. Akbina","userId":"00314994365060536286"}},"outputId":"40ed857d-fecc-49f2-8016-8a8769cecd0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2018 Train verdeling: {0: 9512, 1: 9468}\n","2018 Validation verdeling: {0: 3170, 1: 3157}\n","2018 Test verdeling: {0: 3171, 1: 3156}\n","2019 Train verdeling: {0: 10823, 1: 9640}\n","2019 Validation verdeling: {0: 3608, 1: 3213}\n","2019 Test verdeling: {0: 3608, 1: 3214}\n","2020 Train verdeling: {0: 52657, 1: 43671}\n","2020 Validation verdeling: {0: 17552, 1: 14557}\n","2020 Test verdeling: {0: 17553, 1: 14557}\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1fzQ1-GGFX_0qBuVDkzqbbriOTDYVxb-s","timestamp":1712715972147}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}